# RAG-based Chatbot ğŸ‘¾ 

This project is a Retrieval-Augmented Generation (RAG) chatbot that uses the LLaMA 3.2-3B language model and an embedding system based on FAISS, developed by Meta.

## ğŸ“ŒKey Features 

â¡ï¸Document Retrieval: The chatbot retrieves documents from data sourced via the Europeana API.
â¡ï¸In-Memory Storage: Utilizes an in-memory database built with Redis for fast and efficient data management.
â¡ï¸LangChain Integration: Implements LangChain to build the pipeline (via Hugging Face pipeline), manage prompts, and store vectors using Redis Vector Store.



## ğŸ“‚ Project Structure

ğŸ“¦ repo-name  
â”œâ”€â”€ europeana_data.json   # JSON dataset with data from Europeana  
â”œâ”€â”€ main.py               # Main script to run the chatbot  
â”œâ”€â”€ requirements.txt      # Required libraries  
â”œâ”€â”€ vector_database.py    # Vector database implementation  
â””â”€â”€ README.md  

## âš¡ Utilizzo


## ğŸš€ Installation

Follow these steps to set up the project and run the chatbot locally.

1. Clone the Repository
First, clone the repository to your local machine:
```
git clone https://github.com/your-username/repo-name.git
cd repo-name
```
